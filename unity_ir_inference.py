#!/usr/bin/env python3
"""
Unity IR Inference with Code Leak Steering

Generates natural language Unity behavior JSON from descriptions.
Uses the attractor steering system to detect and correct code leaks.

Usage:
    # Single generation
    python unity_ir_inference.py "rotating coin that gives points when collected"
    
    # Interactive mode
    python unity_ir_inference.py --interactive
    
    # With custom steering intensity
    python unity_ir_inference.py "enemy AI" --intensity 0.7
    
    # Batch processing
    python unity_ir_inference.py --batch prompts.txt --output results/
    
    # Skip steering (raw generation)
    python unity_ir_inference.py "player jump" --no-steering

As module:
    from unity_ir_inference import UnityIRGenerator
    
    generator = UnityIRGenerator()
    result = generator.generate("player that jumps with space")
    print(result.json_output)
"""

import json
import requests
import sys
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
import argparse

# ============================================================================
# IMPORTS FROM MODIFIED PIPELINE
# ============================================================================

from attractor_steering import (
    load_steering,
    AttractorSteering,
    DetectionResult,
    build_natural_language_prompt,
)
from attractor_mapper import (
    detect_code_markers,
    CODE_LEAK_PATTERNS,
)

# ============================================================================
# CONFIGURATION
# ============================================================================

# LLM endpoint (OpenAI-compatible API)
LLM_URL = "http://localhost:1234/v1/chat/completions"
LLM_MODEL = "local-model"

# Steering config (generated by pipeline)
FILTER_CONFIG_DIR = "unity_ir_filter_configs"
MODEL_NAME = "local-model-unity-ir"

# Defaults
DEFAULT_INTENSITY = 0.5
MAX_REGENERATION_ATTEMPTS = 3
DEFAULT_TEMPERATURE = 0.7
DEFAULT_MAX_TOKENS = 2048

# ============================================================================
# SYSTEM PROMPT
# ============================================================================

UNITY_IR_SYSTEM_PROMPT = """You generate Unity game behavior JSON in natural language format.

Output JSON with:
- "class_name": name of the behavior class
- "components": array of Unity component names
- "fields": array of field definitions with name, type, default
- "behaviors": array of behavior objects with name, trigger, condition, actions

CRITICAL: Use NATURAL LANGUAGE, not programming syntax.
- NO operators like ==, <, >, ||
- NO Unity API calls like Vector3.up, Time.deltaTime
- NO function calls like distance(player)
- NO template syntax like {{360 * Time.deltaTime}}
- NO method names like on_trigger_enter

Example GOOD output:
{
  "class_name": "ProximityLight",
  "components": ["Light"],
  "fields": [
    {"name": "detectionRadius", "type": "float", "default": 5}
  ],
  "behaviors": [
    {
      "name": "turn_on",
      "trigger": "player enters detectionRadius",
      "condition": null,
      "actions": [
        {"type": "enable", "params": {"target": "Light"}}
      ]
    },
    {
      "name": "turn_off",
      "trigger": "player exits detectionRadius",
      "condition": null,
      "actions": [
        {"type": "disable", "params": {"target": "Light"}}
      ]
    }
  ]
}

Output ONLY valid JSON, no markdown, no explanations."""

# ============================================================================
# DATA CLASSES
# ============================================================================

@dataclass
class GenerationResult:
    """Result of Unity IR generation"""
    success: bool
    json_output: Optional[str] = None
    parsed: Optional[Dict] = None
    prompt: str = ""
    
    # Steering metadata
    was_steered: bool = False
    attempts: int = 1
    original_output: Optional[str] = None
    
    # Code leak info
    code_leaks_found: List[Dict] = field(default_factory=list)
    detection_result: Optional[DetectionResult] = None
    
    # Errors
    error: Optional[str] = None
    
    def __str__(self) -> str:
        if not self.success:
            return f"GenerationResult(success=False, error={self.error})"
        status = "steered" if self.was_steered else "clean"
        leaks = len(self.code_leaks_found)
        return f"GenerationResult(success=True, status={status}, attempts={self.attempts}, remaining_leaks={leaks})"

# ============================================================================
# LLM INTERFACE
# ============================================================================

def call_llm(
    system_prompt: str,
    user_prompt: str,
    temperature: float = DEFAULT_TEMPERATURE,
    max_tokens: int = DEFAULT_MAX_TOKENS,
) -> Optional[str]:
    """Call local LLM and return response text"""
    try:
        response = requests.post(
            LLM_URL,
            json={
                "model": LLM_MODEL,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": temperature,
                "max_tokens": max_tokens
            },
            timeout=60
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"].strip()
    except Exception as e:
        print(f"LLM call failed: {e}")
        return None


def extract_json_from_response(response: str) -> Tuple[Optional[str], Optional[Dict]]:
    """Extract and parse JSON from LLM response"""
    # Direct parse
    try:
        return response, json.loads(response)
    except json.JSONDecodeError:
        pass
    
    # Find in markdown block
    match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', response)
    if match:
        try:
            json_str = match.group(1)
            return json_str, json.loads(json_str)
        except json.JSONDecodeError:
            pass
    
    # Find raw JSON object
    match = re.search(r'\{[\s\S]*\}', response)
    if match:
        try:
            json_str = match.group(0)
            return json_str, json.loads(json_str)
        except json.JSONDecodeError:
            pass
    
    return None, None

# ============================================================================
# GENERATOR CLASS
# ============================================================================

class UnityIRGenerator:
    """
    Generates Unity IR JSON with code leak steering.
    
    Uses the attractor steering system to detect and correct
    programming syntax that leaks into natural language fields.
    """
    
    def __init__(
        self,
        model_name: str = MODEL_NAME,
        config_dir: str = FILTER_CONFIG_DIR,
        intensity: float = DEFAULT_INTENSITY,
        use_steering: bool = True,
        verbose: bool = False
    ):
        self.model_name = model_name
        self.intensity = intensity
        self.use_steering = use_steering
        self.verbose = verbose
        self.steering: Optional[AttractorSteering] = None
        
        if use_steering:
            try:
                self.steering = load_steering(model_name, config_dir)
                if verbose:
                    n_attractors = len(self.steering.config.attractors)
                    print(f"Loaded steering: {n_attractors} attractors")
            except FileNotFoundError:
                print(f"Warning: No steering config at {config_dir}/{model_name}/")
                print("Run pipeline first to generate configs, or use --no-steering")
    
    def generate(
        self,
        behavior_description: str,
        temperature: float = DEFAULT_TEMPERATURE,
        max_attempts: int = MAX_REGENERATION_ATTEMPTS
    ) -> GenerationResult:
        """Generate Unity IR JSON from behavior description"""
        
        result = GenerationResult(success=False, prompt=behavior_description)
        
        if self.verbose:
            print(f"Generating: {behavior_description}")
        
        # Initial generation
        response = call_llm(UNITY_IR_SYSTEM_PROMPT, behavior_description, temperature)
        
        if response is None:
            result.error = "LLM call failed"
            return result
        
        json_str, parsed = extract_json_from_response(response)
        
        if json_str is None:
            result.error = "Failed to extract valid JSON"
            result.original_output = response
            return result
        
        result.original_output = json_str
        
        # Detect code leaks using pipeline's detector
        code_leaks = detect_code_markers(json_str)
        result.code_leaks_found = code_leaks
        
        # Full detection with steering if available
        needs_correction = len(code_leaks) > 0
        
        if self.steering:
            detection = self.steering.detect_code_leak(json_str, intensity=self.intensity)
            result.detection_result = detection
            needs_correction = detection.is_attracted
            
            if self.verbose:
                print(f"  Leak score: {detection.keyword_score}, triggered: {detection.is_attracted}")
        
        # Return clean output
        if not needs_correction:
            result.success = True
            result.json_output = json_str
            result.parsed = parsed
            return result
        
        # Correction loop
        if self.verbose:
            print(f"  Found {len(code_leaks)} leaks, correcting...")
        
        result.was_steered = True
        corrected_json = json_str
        current_leaks = code_leaks
        
        for attempt in range(max_attempts):
            result.attempts = attempt + 2
            
            # Build correction prompt using pipeline's function
            flagged = [leak["pattern"] for leak in current_leaks[:10]]
            system_prompt, user_prompt = build_natural_language_prompt(
                corrected_json, 
                flagged
            )
            
            # Generate correction
            corrected_response = call_llm(
                system_prompt,
                user_prompt,
                temperature=max(0.3, temperature - 0.2)
            )
            
            if corrected_response is None:
                continue
            
            new_json, new_parsed = extract_json_from_response(corrected_response)
            
            if new_json is None:
                continue
            
            # Check improvement
            new_leaks = detect_code_markers(new_json)
            
            if self.verbose:
                print(f"  Attempt {attempt + 1}: {len(current_leaks)} â†’ {len(new_leaks)} leaks")
            
            if len(new_leaks) < len(current_leaks):
                corrected_json = new_json
                parsed = new_parsed
                current_leaks = new_leaks
            
            if len(new_leaks) == 0:
                break
        
        result.success = True
        result.json_output = corrected_json
        result.parsed = parsed
        result.code_leaks_found = current_leaks
        
        return result
    
    def generate_batch(self, descriptions: List[str], temperature: float = DEFAULT_TEMPERATURE) -> List[GenerationResult]:
        """Generate Unity IR for multiple descriptions"""
        results = []
        for i, desc in enumerate(descriptions):
            if self.verbose:
                print(f"\n[{i+1}/{len(descriptions)}] {desc[:50]}...")
            results.append(self.generate(desc, temperature))
        return results

# ============================================================================
# CLI MODES
# ============================================================================

def interactive_mode(generator: UnityIRGenerator):
    """Interactive generation session"""
    print("=" * 60)
    print("UNITY IR GENERATOR - Interactive Mode")
    print("=" * 60)
    print("Enter behavior descriptions. Commands: quit, intensity <0-1>, verbose, quiet")
    print("=" * 60)
    
    while True:
        try:
            prompt = input("\n> ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nExiting.")
            break
        
        if not prompt:
            continue
        if prompt.lower() == "quit":
            break
        if prompt.lower().startswith("intensity "):
            try:
                generator.intensity = float(prompt.split()[1])
                print(f"Intensity: {generator.intensity}")
            except (ValueError, IndexError):
                print("Usage: intensity <0-1>")
            continue
        if prompt.lower() == "verbose":
            generator.verbose = True
            print("Verbose on")
            continue
        if prompt.lower() == "quiet":
            generator.verbose = False
            print("Verbose off")
            continue
        
        result = generator.generate(prompt)
        
        if result.success:
            print(f"\n{result}")
            print("-" * 40)
            print(result.json_output)
            if result.was_steered and result.code_leaks_found:
                print("-" * 40)
                print(f"Remaining leaks: {[l['pattern'] for l in result.code_leaks_found]}")
        else:
            print(f"\nFailed: {result.error}")


def batch_mode(generator: UnityIRGenerator, input_file: str, output_dir: str):
    """Process prompts from file"""
    input_path = Path(input_file)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    with open(input_path) as f:
        prompts = [l.strip() for l in f if l.strip() and not l.startswith("#")]
    
    print(f"Processing {len(prompts)} prompts...")
    results = generator.generate_batch(prompts)
    
    # Save individual JSONs
    for i, (prompt, result) in enumerate(zip(prompts, results)):
        if result.success and result.parsed:
            safe_name = "".join(c if c.isalnum() else "_" for c in prompt[:40])
            with open(output_path / f"{i:03d}_{safe_name}.json", "w") as f:
                json.dump(result.parsed, f, indent=2)
    
    # Save summary
    summary = {
        "total": len(results),
        "successful": sum(1 for r in results if r.success),
        "steered": sum(1 for r in results if r.was_steered),
        "failed": sum(1 for r in results if not r.success),
        "results": [
            {
                "prompt": r.prompt,
                "success": r.success,
                "steered": r.was_steered,
                "attempts": r.attempts,
                "remaining_leaks": len(r.code_leaks_found),
                "error": r.error
            }
            for r in results
        ]
    }
    
    with open(output_path / "summary.json", "w") as f:
        json.dump(summary, f, indent=2)
    
    print(f"\nSaved to {output_path}/")
    print(f"  Success: {summary['successful']}/{summary['total']}")
    print(f"  Steered: {summary['steered']}")
    print(f"  Failed: {summary['failed']}")

# ============================================================================
# MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Generate Unity IR JSON from behavior descriptions"
    )
    parser.add_argument("prompt", nargs="?", help="Behavior description")
    parser.add_argument("--interactive", "-i", action="store_true")
    parser.add_argument("--batch", "-b", metavar="FILE")
    parser.add_argument("--output", "-o", default="unity_ir_output")
    parser.add_argument("--intensity", type=float, default=DEFAULT_INTENSITY)
    parser.add_argument("--no-steering", action="store_true")
    parser.add_argument("--verbose", "-v", action="store_true")
    parser.add_argument("--model", default=MODEL_NAME)
    parser.add_argument("--config-dir", default=FILTER_CONFIG_DIR)
    
    args = parser.parse_args()
    
    generator = UnityIRGenerator(
        model_name=args.model,
        config_dir=args.config_dir,
        intensity=args.intensity,
        use_steering=not args.no_steering,
        verbose=args.verbose
    )
    
    if args.interactive:
        interactive_mode(generator)
    elif args.batch:
        batch_mode(generator, args.batch, args.output)
    elif args.prompt:
        result = generator.generate(args.prompt)
        if result.success:
            print(result.json_output)
            if args.verbose:
                print(f"\n---\n{result}", file=sys.stderr)
        else:
            print(f"Error: {result.error}", file=sys.stderr)
            sys.exit(1)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()

